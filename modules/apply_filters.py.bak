import numpy as np
import logging
from scipy.interpolate import interp1d
from modules.crs_registry import set_crs
from modules.data_types import point_with_normals_dtype  # ✅ Ensure correct dtype

logger = logging.getLogger(__name__)

def apply_filters(chunk, traj_data, config):
    """
    Applies range, motion, and Z filters to a point cloud chunk based on sensor trajectory data.
    Computes pseudo-normals for all points based on the sensor trajectory.

    Args:
        chunk (np.ndarray): Structured array containing point cloud data.
        traj_data (np.ndarray): Structured NumPy array of sensor trajectory data.
        config (JSONRegistry): The runtime configuration object.

    Returns:
        pass_chunk (np.ndarray): Points passing all filtering conditions (with pseudo-normals).
        fail_chunk (np.ndarray): Points that fail one or more filtering conditions (with pseudo-normals).
    """

    logger.debug("🔹 Filtering in the SENSOR frame...")

    # --- Extract filtering parameters from JSON config ---
    short_range = config.get("filtering.range_min")
    long_range = config.get("filtering.range_max")
    min_motion = config.get("filtering.motion_threshold")
    z_pass_offset = config.get("filtering.z_pass_offset")

    # --- Extract point cloud attributes ---
    point_x = chunk["x"]
    point_y = chunk["y"]
    point_z = chunk["z"]
    point_range = chunk["range"]
    point_time = chunk["time"]

    # --- Extract trajectory attributes (handling field name variations) ---
    if "traj_x" in traj_data.dtype.names:  # SENSOR TRAJECTORY
        traj_x = traj_data["traj_x"]
        traj_y = traj_data["traj_y"]
        traj_z = traj_data["traj_z"]
    elif "local_x" in traj_data.dtype.names:  # LOCAL TRAJECTORY (GNSS)
        traj_x = traj_data["local_x"]
        traj_y = traj_data["local_y"]
        traj_z = traj_data["local_z"]
    else:
        raise ValueError("❌ apply_filters: No valid trajectory fields found!")

    traj_time = traj_data["time"]

    logger.debug(f"🔹 Point Time Min-Max: {point_time.min()} - {point_time.max()}")
    logger.debug(f"🔹 Traj Time Min-Max: {traj_time.min()} - {traj_time.max()}")

    # --- Interpolate trajectory X, Y, Z values at point times ---
    traj_x_interp = interp1d(traj_time, traj_x, kind="linear", fill_value="extrapolate")
    traj_y_interp = interp1d(traj_time, traj_y, kind="linear", fill_value="extrapolate")
    traj_z_interp = interp1d(traj_time, traj_z, kind="linear", fill_value="extrapolate")

    interpolated_traj_x = traj_x_interp(point_time)
    interpolated_traj_y = traj_y_interp(point_time)
    interpolated_traj_z = traj_z_interp(point_time)

    logger.debug(f"🔹 Interpolated Traj Z Shape: {interpolated_traj_z.shape}")

    # --- Compute motion distances from trajectory ---
    traj_diff = np.diff(np.column_stack((traj_x, traj_y, traj_z)), axis=0)
    motion_distances = np.linalg.norm(traj_diff, axis=1)
    motion_distances = np.insert(motion_distances, 0, 0)  # Insert zero for first entry

    # --- Interpolate motion distances at point times ---
    motion_interp = interp1d(traj_time, motion_distances, kind="linear", fill_value="extrapolate")
    interpolated_motion = motion_interp(point_time)

    # --- Apply range filtering ---
    valid_range = (short_range < point_range) & (point_range < long_range)

    # --- Apply motion filtering ---
    valid_motion = interpolated_motion >= min_motion

    # --- Apply Z filtering (ensuring points are above a dynamic "pseudo floor") ---
    pseudo_floor = interpolated_traj_z + z_pass_offset
    valid_z = point_z > pseudo_floor

    logger.debug(f"🔹 Z-Pass Threshold: {z_pass_offset}")
    logger.debug(f"🔹 Valid Range Count: {np.sum(valid_range)}")
    logger.debug(f"🔹 Valid Motion Count: {np.sum(valid_motion)}")
    logger.debug(f"🔹 Valid Z Count: {np.sum(valid_z)}")

    # --- Create masks for pass/fail conditions ---
    pass_mask = valid_range & valid_motion & valid_z
    fail_mask = ~pass_mask  # ✅ Fail points still get pseudo-normals!

    # --- Compute Pseudo-Normals for **All Points** ---
    sensor_vectors = np.column_stack((
        interpolated_traj_x - point_x,
        interpolated_traj_y - point_y,
        interpolated_traj_z - point_z
    ))

    # Normalize vectors to get unit normals
    norms = np.linalg.norm(sensor_vectors, axis=1, keepdims=True)
    pseudo_normals = sensor_vectors / np.maximum(norms, 1e-6)  # Prevent division by zero

    logger.debug("✅ Pseudo-Normals Calculated for All Points")

    # --- Create Output Structured Arrays with Pseudo-Normals ---
    pass_chunk = np.empty(np.sum(pass_mask), dtype=point_with_normals_dtype)
    fail_chunk = np.empty(np.sum(fail_mask), dtype=point_with_normals_dtype)

    # ✅ Assign values to pass_chunk
    for field in ["time", "x", "y", "z", "intensity", "ring", "returnNum", "range"]:
        pass_chunk[field] = chunk[field][pass_mask]

    pass_chunk["nx"] = pseudo_normals[pass_mask, 0]
    pass_chunk["ny"] = pseudo_normals[pass_mask, 1]
    pass_chunk["nz"] = pseudo_normals[pass_mask, 2]

    # ✅ Assign values to fail_chunk (now includes pseudo-normals)
    for field in ["time", "x", "y", "z", "intensity", "ring", "returnNum", "range"]:
        fail_chunk[field] = chunk[field][fail_mask]

    fail_chunk["nx"] = pseudo_normals[fail_mask, 0]
    fail_chunk["ny"] = pseudo_normals[fail_mask, 1]
    fail_chunk["nz"] = pseudo_normals[fail_mask, 2]

    # ✅ Update CRS Registry to indicate Pseudo-Normals Exist
    set_crs("has_pseudo_normals", True)  # ✅ Use True instead of 1 for consistency

    logger.info(f"✅ Total Points Kept (PASS): {len(pass_chunk)}")
    logger.info(f"✅ Total Points in FAIL bucket: {len(fail_chunk)}")

    return pass_chunk, fail_chunk
