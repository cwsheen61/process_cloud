#!/usr/bin/env python3
"""
Processes a large PLY point cloud using a dense sensor trajectory and a sparse GNSS trajectory
to compute a 3D transformation (rotation about Z and translation). The transformed data is
optionally converted to a target CRS, streamed in chunks, and stored in LAZ format.
"""

# --- Global Imports ---
import sys
import os
import json
import logging
import shutil
import inspect
import multiprocessing
import numpy as np
from modules.json_registry import JSONRegistry
from modules.load_trajectory import load_trajectory
from modules.load_gnss_trajectory import load_gnss_trajectory
from modules.compute_global_transform import compute_global_transform
from modules.prepare_temp_directory import prepare_temp_directory
from modules.load_ply_chunks import load_ply_chunks
from modules.process_chunk import process_chunk
from modules.merge_laz_files import merge_laz_files

# --- Logging Configuration ---
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s | %(levelname)s | %(message)s (%(filename)s:%(lineno)d)")
logger = logging.getLogger(__name__)

def lineno():
    """Returns the current line number in our program."""
    import inspect
    return inspect.currentframe().f_back.f_lineno

def main():
    """
    Main execution pipeline for processing point cloud data using GNSS trajectories.
    """
    logger.info("üîç Loading JSON configuration... (%s:%d)", __file__, lineno())

    # --- Survey sys.argv for file paths ---
    ply_file_path = None
    json_file_path = None
    for arg in sys.argv[1:]:
        if arg.lower().endswith(".ply"):
            ply_file_path = arg
        elif arg.lower().endswith(".json"):
            json_file_path = arg
    if not ply_file_path or not json_file_path:
        logger.error("‚ùå Missing required file arguments. Provide both a .ply and a .json file. (%s:%d)",
                     __file__, lineno())
        sys.exit(1)

    # --- Extract base paths ---
    ply_path, ply_file = os.path.split(ply_file_path)
    config_path, config_file = os.path.split(json_file_path)
    logger.info(f"üìÇ Detected ply_path: {ply_path}, ply_file: {ply_file}")
    logger.info(f"üìÇ Detected config_path: {config_path}, config_file: {config_file}")

    # --- Copy seed config to current_config.json ---
    current_config_path = os.path.join(ply_path, "current_config.json")
    try:
        shutil.copy(json_file_path, current_config_path)
        logger.info("‚úÖ Created `current_config.json` from seed config. (%s:%d)", __file__, lineno())
    except Exception as e:
        logger.error("‚ùå Failed to create `current_config.json`: %s (%s:%d)", e, __file__, lineno())
        sys.exit(1)

    # --- Populate current_config.json with dynamic paths ---
    working_config_file = os.path.join(ply_path, "current_config.json")
    logger.info(f"üìÑ Using working configuration file: {working_config_file} ({__file__}:{lineno()})")
    config = JSONRegistry(working_config_file, working_config_file)
    logger.info(f"‚úÖ Reloaded config from updated file: {working_config_file}")

    try:
        config.set("pathname", ply_path)
        config.set("files.ply", os.path.join(ply_path, ply_file_path))
        # Resolve trajectory file path
        traj_path = config.get("files.trajectory", "trajectory.txt")
        if not os.path.isabs(traj_path):
            traj_path = os.path.join(ply_path, traj_path)
        config.set("files.trajectory", traj_path)
        # Resolve GNSS trajectory file path
        gnss_path = config.get("files.gnss_trajectory", "gnss.txt")
        if not os.path.isabs(gnss_path):
            gnss_path = os.path.join(ply_path, gnss_path)
        config.set("files.gnss_trajectory", gnss_path)
        # Resolve other file paths similarly
        transformed_traj = config.get("files.transformed_trajectory", "transformed_traj.txt")
        if not os.path.isabs(transformed_traj):
            transformed_traj = os.path.join(ply_path, transformed_traj)
        config.set("files.transformed_trajectory", transformed_traj)
        output_pass = config.get("files.output_pass", "output_pass.laz")
        if not os.path.isabs(output_pass):
            output_pass = os.path.join(ply_path, output_pass)
        config.set("files.output_pass", output_pass)
        output_fail = config.get("files.output_fail", "output_fail.laz")
        if not os.path.isabs(output_fail):
            output_fail = os.path.join(ply_path, output_fail)
        config.set("files.output_fail", output_fail)
        config.save()
        logger.info(f"‚úÖ Updated `current_config.json` with dynamic paths: {working_config_file}")
    except Exception as e:
        logger.error(f"‚ùå Error updating `current_config.json`: {e}")
        sys.exit(1)

    # --- Reload updated config for further processing ---
    config = JSONRegistry(current_config_path, current_config_path)
    logger.info("‚úÖ Initialized current config: %s (%s:%d)", current_config_path, __file__, lineno())

    # --- Prepare working directory ---
    temp_dir = prepare_temp_directory(ply_path)
    logger.info("üìÇ Temp directory initialized: %s (%s:%d)", temp_dir, __file__, lineno())

    # --- Load trajectory data (sensor) ---
    trajectory_data = load_trajectory(current_config_path)
    traj_file = config.get("files.trajectory")
    logger.info("‚úÖ Loaded trajectory: %s (%s:%d)", traj_file, __file__, lineno())

    # --- Load GNSS trajectory data ---
    gnss_file = config.get("files.gnss_trajectory")
    if not gnss_file:
        logger.error("‚ùå GNSS trajectory file is missing from config! (%s:%d)", __file__, lineno())
        sys.exit(1)
    gnss_data = load_gnss_trajectory(current_config_path)
    logger.info("‚úÖ Loaded GNSS trajectory from %s (%s:%d)", gnss_file, __file__, lineno())

    # --- Compute global transformation ---
    compute_global_transform(current_config_path)
    logger.info("‚úÖ Global transformation computed and stored in config (%s:%d)", __file__, lineno())

    # --- Reload config to get transformation parameters ---
    config = JSONRegistry(current_config_path, current_config_path)
    R_global = np.array(config.get("transformation.R"))
    t_global = np.array(config.get("transformation.t"))
    crs_epsg = config.get("crs.epsg")
    logger.info("‚úÖ Transformation parameters: R=%s, t=%s, EPSG=%s", R_global, t_global, crs_epsg)

    # --- Load point cloud in chunks ---
    logger.info("üîÑ Loading point cloud chunks from %s (%s:%d)", ply_file_path, __file__, lineno())
    chunks = load_ply_chunks(current_config_path)  # load_ply_chunks extracts needed info from config

    # --- Build argument tuples for each chunk ---
    args = []
    for idx, chunk in enumerate(chunks):
        args.append((idx, chunk, trajectory_data, R_global, t_global, crs_epsg, temp_dir, config))

    num_workers = config.get("processing.num_workers", multiprocessing.cpu_count())
    logger.info("üöÄ Processing %d chunks using %d workers. (%s:%d)", len(args), num_workers, __file__, lineno())

    # --- Process chunks using multiprocessing ---
    with multiprocessing.Pool(processes=num_workers) as pool:
        results = pool.map(process_chunk, args)

    # Unpack results (each is a tuple: (pass_filename, fail_filename))
    try:
        pass_chunks, fail_chunks = zip(*results)
    except ValueError:
        pass_chunks, fail_chunks = (), ()
    logger.info("‚úÖ Processing complete. Passed: %d, Failed: %d (%s:%d)",
                len(pass_chunks), len(fail_chunks), __file__, lineno())

    # --- Merge LAZ files ---
    merge_laz_files(ply_path, config)
    logger.info("‚úÖ LAZ merging completed (%s:%d)", __file__, lineno())

if __name__ == "__main__":
    main()
